---
title: "Ethics and Text Analysis"
teaching: 20
exercises: 20
questions:
- "todo"
objectives:
- "todo"
keypoints:
- "todo"
---


## Is text analysis artificial intelligence? 

Artificial intelligence is loosely defined as the ability for computer systems to perform tasks that have traditionally required human reasoning and perception. 
* To the extent that text analysis performs a task that resembles reading, understanding, and analyzing meaning, it can be understood to be part of the definition of artificial intelligence. 
* The methods in this lesson all demonstrate models that learn from data - specifically, from text corpora that are not structured to explicitly tell the machine anything other than, perhaps, title, author, date, and body of text.
* As a method and a tool, it is important to understand the tasks to which it is best suited, and to understand the process well enough to be able to interpret the results, including:

  1. whether they are relevant 
  2. whether they have been overly influenced by the model or training data 
  3. and how to use the results 

## How to determine relevance (?)
Any good way to address this? Maybe this is more of a research methods issues, than a tools or AI issue?

## How the training data can influence results

There are numerous examples of how training data - or the language model, ultimately - can negatively influence results. Reproducing bias in the data is probably one of the most discussed negative outcomes. Let's look at one real world example:
* Loomis v. Wisconsin: This case involved the use of "algorithmic risk assessments" to assist in sentencing offenders, and considered the proprietary nature of the tool used to determine outcomes and issues with race and gender being factors weighed in the risk assessment. The same  year that the case was brought in Wisconsin (State v. Loomis, 2016), ProPublica published an investigative report that exposed the clear bias against Black people in computer programs used to determine the likelihood of defendants committing crimes in the future. That bias was built into the tool because the training data that it relied on included historical data about crime statistics, which reflected - and then reproduced - existing racist bias in sentencing. 
  1. This case illustrates how training data can negtively influence results and reproduce bias. 

   * How does is demonstrate interpreting the relevance of the results? 
   * Is there any responsible way to use the results of a tool where the training data has been shown to negatively influenced the results?


Resources: 
* State v. Loomis: Wisconsin Supreme Court Requires Warning Before Use of Algorithmic Risk Assessments in Sentencing. Harvard Law Review, March 10, 2017. [https://harvardlawreview.org/2017/03/state-v-loomis/](https://harvardlawreview.org/2017/03/state-v-loomis/)
* Julia Angwin, Jeff Larson, Surya Mattua, and Lauren Kirchner. Machine Bias. ProPublica, May 23, 2016. [https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
* Stanford Human Centered Artificial Intelligence: [Definitions](https://hai.stanford.edu/sites/default/files/2020-09/AI-Definitions-HAI.pdf)


???
## Black box AI and understanding processes...

## Humans in the loop and AI as assistant, not authority
~~~
code goes here
~~~
